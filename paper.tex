\documentclass[sigconf]{acmart}
\newcommand{\wmnote}[1]{{\scriptsize \color{red} [[ Billy: #1]]}}
\newcommand{\gsnote}[1]{{\scriptsize \color{blue} [[ George: #1]]}}
\newcommand{\pmnote}[1]{{\scriptsize \color{magenta} [[ Pat: #1]]}}

\usepackage{booktabs} % For formal tables
\usepackage{multicol}
\usepackage{minted}
\graphicspath{{./figs/}}

% Dejavu fonts have unicode character support
% \usepackage{DejaVuSansMono}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{listings}
\usepackage{subfigure}
\newcommand{\figlabel}[1]   {\label{fig:#1}}
\newcommand{\figref}[1]         {Figure~\ref{fig:#1}}
\newcommand{\figreftwo}[2]      {Figures \ref{fig:#1} and~\ref{fig:#2}}
\newcommand{\figrefthree}[3]      {Figures \ref{fig:#1}, \ref{fig:#2} and~\ref{fig:#3}}
\newcommand{\subfiglabel}[1]    {\textbf{#1}}
\newcommand{\subfigcap}[1]      {\textbf{~~#1}}
\newcommand{\subfigref}[2]      {Figure~\ref{fig:#1}#2}
\newcommand{\subfigreftwo}[3]      {Figures~\ref{fig:#1}#2 and~\ref{fig:#1}#3}

\usepackage{tikz}
\usetikzlibrary{decorations.pathreplacing,shapes,arrows,positioning,calc}

\tikzset{
  basic block/.style={
    rectangle,
    rounded corners,
    draw=black, thin,
    % minimum width=3.2cm,
    inner sep=3pt,
    % outer sep=4pt,
    align=left,
    anchor=west,
    fill=codebgcolor, 
    % font={\ttfamily\fontsize{8.5}{10.2}\selectfont},
    font={\ttfamily\fontsize{7.5}{9}\selectfont},
    text=terminalcolor,
    draw=codeheadcolor,
    thick,
    auto,
    node distance=.4cm and 1.25cm,
  },
  cfedge/.style={
    font=\itshape,
    draw=black,
    ->,
    >=stealth'
  },
  process/.style={
    draw,
    fill=orange!50,
    rectangle,
    minimum height=1.5em,
    minimum width=5em,
    align=center,
    font=\small,
  },
  form/.style={
    draw,
    fill=blue!30,
    ellipse,
    minimum height=1em,
    align=center,
    font=\footnotesize,
  },
  slave/.style={
    execute at end picture={
      \coordinate (lower right) at (current bounding box.south east);
      \coordinate (upper left) at (current bounding box.north west);
      \pgfresetboundingbox
      \path (upper left) rectangle (lower right);
    }
  }
}

\definecolor{CodeColor}{RGB}{98,39,26}
\def\code{\lstinline[basicstyle=\ttfamily\color{CodeColor}]} 


\usepackage{ucs}
\usepackage[utf8x]{inputenc}
\usepackage{autofe}

\lstset{numbers=left,
        numberstyle=\tiny,
        numbersep=5pt,
        xleftmargin=0.1in
}

% Copyright
%\setcopyright{none}
%\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
%\settopmatter{printacmref=false}
\setcopyright{rightsretained}
%\setcopyright{usgov}
%\setcopyright{usgovmixed}
%\setcopyright{cagov}
%\setcopyright{cagovmixed}

% DOI
\acmDOI{10.475/123_4}

% ISBN
\acmISBN{123-4567-24-567/08/06}

%Conference
\acmConference[LLVM in HPC'17]{LLVM in HPC: Fourth Workshop on the LLVM Compiler Infrastructure in HPC}{November 2017}{Denver Colorado, USA} 
\acmYear{2017}
\copyrightyear{2017}

%\acmPrice{15.00}

\begin{document}
\lstset{basicstyle=\tt\small, language=C, 
morekeywords={omp, for, pragma, omp, task, taskwait}}

\title{OpenMPIR}
\subtitle{Implementing OpenMP tasks with Tapir}
\author{George Stelle}
\affiliation{\institution{Los Alamos National Laboratory}}
\email{stelleg@lanl.gov}

\author{William S. Moses}
\affiliation{\institution{MIT CSAIL}}
\email{wmoses@mit.edu}

\author{Stephen Olivier}
\affiliation{\institution{Center for Computing Research \\ Sandia National Laboratories}}
\email{slolivi@sandia.gov}

\author{Pat McCormick}
\affiliation{\institution{Los Alamos National Laboratory}}
\email{pat@lanl.gov}

\begin{abstract}
\pmnote{Overhauled this some -- take a look...}
Optimizing compilers for task-level parallelism are still in their infancy.
This work explores a frontend that compiles OpenMP tasking semantics to Tapir, 
an extension to LLVM IR that represents fork-join parallelism. This enables
analyses and optimizations that were previously inaccessible to OpenMP codes, as well as
the ability to target additional runtimes at code generation. Using a Cilk runtime 
backend, we compare results to existing OpenMP implementations. Initial performance 
results for the Barcelona OpenMP tasking suite and show performance improvements over 
these existing implementations.
\end{abstract}

\maketitle

\section{Introduction} \label{Sec:Introduction}
 
When writing task-parallel programs today, one has a large selection of
potential programming models and implementations to consider. Unfortunately, 
despite having some significant overlaps in semantics, parallel programming
models such as OpenMP~\cite{openmp}, Cilk~\cite{cilk}, Kokkos~\cite{kokkos},
HPX~\cite{hpx}, Charm++~\cite{charm}, Qthreads~\cite{qthreads},
pthreads\cite{pthreads}, MPI~\cite{mpi}, Chapel~\cite{chapel}, UPC~\cite{upc},
OpenCL~\cite{opencl}, OpenACC~\cite{openacc} among others have limited ability to
interoperate either at the level of compile-time analysis or at run-time
execution.  \wmnote{Not sure if relevant, but there was some experiment run a
while back of someone composing a library that internally used OpenMP with
another framework that had terribly results compared with either used alone.}
\gsnote{Hmm. A quick googling turned up a project out of berkeley combining
OpenMP and TBB, but performance didn't seem too bad. I'll take another look
tomorrow.} In addition, many of these programming models are implemented as
runtime libraries with only primitive compiler support. As a result, most are
not able to take advantage of the compiler's analysis and optimization
capabilities.  While there has been some recent work on specializing
compilers to reason about parallel programming libraries~\cite{Moss_2016},
fragmentation among models presents a significant challenge for compiler
writers, which must choose a single model, or multiple intermediate forms, to
use for analysis and optimization. An ideal solution would be a form of
common representation of parallel semantics and constructs.

Compilers implement the majority of their analyses and optimizations on
intermediate representations \textbf{(IRs)} which allow such transformations
to be written once with relative ease and apply to a variety of source
front-end languages such as C or C++.  Historically, the IR of mainstream
compilers such as LLVM~\cite{llvm} or GCC's Gimple~\cite{} haven't suppored
parallel constructs. As a consequence, compilers utilizing these IRs haven't
had the ability to reason about parallelism. The recent work of Schardl et
al.~\cite{tapir} has shown that the LLVM IR can be extended to represent
fork-join parallelism without requiring a major rewrite.  They did so by
extending the LLVM instruction set with three instructions capable of
representing fork-join parallelism. To benchmark performance and test accuracy,
they wrote a frontend which translated programs written with Cilk, a parallel
programming model for C/C++, into the Tapir IR. They also implemented a backend
to \textbf{lower} Tapir programs to vanilla LLVM IR with embedded Cilk runtime
calls. They suggested that a similar approach could be taken with OpenMP and
other such frameworks.

In this work, we put that suggestion to the test, implementing OpenMP tasks by
compiling them to Tapir instructions. This allows us to both analyze and optimize
OpenMP programs as well as compile programs written in OpenMP to use other runtimes.
For this paper, our prototype implementation uses the Cilk runtime as a backend,
and we run our prototype implementation on the Barcelona OpenMP task suite~\cite{barcelona}. 
We discuss the kinds of optimizations this enables, and how
the work can be extended to include other parallelism constructs and semantics
in OpenMP. We also discuss the possibility of extending this work to other
programming models, which would help to alleviate some of the fragmentation
issues discussed above. 

The contributions of this work include: 

\begin{itemize}
  \item A prototype frontend that transforms OpenMP task constructs to Tapir IR.
  \item Benchmarks of the implementation using a Cilk runtime backend, showing
        performance improvements over existing OpenMP implementations.
  \item An initial investigation into the reasons for these performance improvements.
\end{itemize}

Our hope for this paper is to move the community towards further conversations on a 
shared IR that can be used for many programming models. The work presented in this paper only
represents a small step in that direction and there is still a herculean amount
of design and implementaton details to be considered. We argue that the benefits vastly outweigh 
the costs and we hope to see additional future work explore additional possibilities. 

% I'd skip the subsection...  Not critical, just personal prefs... 
% \subsection{Outline}

The remainder of the paper is organized as follows: In Section~\ref{Sec:Background} 
we give background and motivation for the paper. Following that, we describe OpenMP
tasking in Section~\ref{Sec:OpenMP}, and Tapir in Section~\ref{Sec:Tapir}. We then 
discuss the implementation, and how we compile OpenMP task constructs to Tapir IR in
Section~\ref{Sec:Implementation}. In Section~\ref{Sec:Evaluation} we describe the 
evaluation setup, including what implementations we compare to and how. We discuss
the results of evaluation in Section~\ref{Sec:Results}, including possible
explanations for discrepancies. We discuss the implications of the results ans the
significance of this work in a large context in~\ref{Sec:Discussion}. Finally,
we cover some of the many ways the work can be extended in
Section~\ref{Sec:Future}, and conclude in Section~\ref{Sec:Conclusion}.

\section{Background} \label{Sec:Background}

Internal representations are a crucial part of modern compiler design and implementation.  By
representing code in a generic, hardware-agnostic format, they allow both
multiple frontends and backends to all take advantage of a common/shared series of compiler 
analyses and optimizations. 

LLVM has gained traction in the broad community for its simplicity,
relative ease of both experimentation and modifications, and extensibility~\cite{lattner2004llvm}.
Historically, LLVM has only had a sequential semantics: there is no explicit notion
of concurrent or parallel processes. This has resulted in existing compilers
treating parallelism constructs as thin wrappers for calls into runtime
systems. These calls are infeasible for the compiler to reason about, due to
the complexity and flexibility of the runtimes being called \pmnote{and the challenge of
reasoning about the corresponding semantics in a uniform fashion?}. The addition of parallel
constructs were recently explored by Schardl et al.~\cite{tapir}. By adding three first-class 
instructions to LLVM, Tapir enables analyses and optimizations of parallel constructs. We briefly 
describe the key aspects of Tapir in Section~\ref{Sec:Tapir}.
% I removed this as the reference was already given...   PM 
% For a more complete description see \cite{tapir}. 

Arguably the most ubiquitous parallel programming model for on-node parallelism
in HPC, OpenMP has grown from being a way to implement parallel loops into a
large and complex specification for parallelism. This paper focuses on a 
relatively recent addition to the OpenMP specification: the \texttt{task}
and \texttt{taskwait} constructs. A detailed description of these constructs
is presented in Section~\ref{Sec:OpenMP} and a more complete description can 
be found in the OpenMP specification~\cite{openmp}.

\subsection{Fragmentation}

In this section we further discuss a significant challenge in HPC:
fragmentation of parallel programming models. There are many ways to write
parallel programs~\cite{openmp, qthreads, chapel, cilk, kokkos, legion, upc, mpi}
\pmnote{I would add MPI and UPC/PGAS here too}. From libraries, to language
extensions, to standalone languages, to combinations of the above,
fragmentation of parallel programming models is a problem that is only getting
worse. This means that when writing tooling, optimizations, or analyses, one
has to choose which model to target as well as understand points of contention
and interoperability between them. A common difficulty among all of these is
that reasoning about parallel programs is hard and compilers can potentially
help reduce this burden. This is exactly analogous the problem LLVM solves:
compiler optimizations are hard, so sharing them is valuable. Tapir is a
demonstration that the the LLVM approach to compilation of serial code can be
effectively extended to help reason about parallel code. 

With fragmentation comes questions of compatibility, or composability, of 
different programming models. To illustrate this issue, consider the example of
a program using a library that uses OpenMP to implement parallelism internally. 
If said program uses a different programming model for parallelism, e.g. Cilk,
there is currently no good solution for ensuring that the two different backends
will cooperate with management of hardware resources. 

Addressing these concerns this is a major motivation for our use of Tapir.
\pmnote{This made me remember that Rice and Cray folks did some work on extending LLVM
to reason about PGAS-like constructs We should probably call that out in 
this discussion -- that said, I don't recall their approach being general so it 
does not quite fit the discussion here but I'd have to think a bit harder if it
would be possible to capture something like MPI operations in some fashion... 
I will email George a pointer to the paper...} 
By standardising an IR, in addition to easing the development of optimizations,
one avoids duplicating work across different implementations. It also enables
the possibility of multiple programming models coexisting peacefully. We'll
return to this issue later in Section~\ref{Sec:Discussion} when discussing
future work.

\section{Tapir} \label{Sec:Tapir}
Tapir is an extension to LLVM proposed by Schardl, Moses, and Leiserson
aiming to resolve issues regarding optimizing parallel code. Prior to
the introduction of Tapir, compilers would represent parallel programs
by directly translating their syntax to opaque runtime calls. This allowed
for compilers to support parallel programs, but meant that traditional 
optimizations such as code motion \cite{} or common-subexpression elimination \cite{}
weren't able to reason about parallel programs. This often led to parallel
programs running significantly slower than expected. In the Tapir project,
the authors show that it is possible to represent and optimize fork-join
parallel programs with relatively minimal modifications to the compiler: namely
the introduction of three instructions designed to interface well with an
existing compiler.

\subsection{Compilation without Tapir}

\begin{figure}[t]
\begin{tabular*}{\linewidth}{@{\extracolsep{\fill}}l@{}l}

\subfiglabel{a}\\
\begin{lstlisting}
void search(int low, int high) {
  if (low == high) search_base(low);
  else {
    #pragma omp task
    search(low, (low+high)/2);
    #pragma omp task
    search((low+high)/2 + 1, high);
    #pragma omp taskwait
  } 
}
\end{lstlisting}
\\
\subfiglabel{b}\\
\begin{lstlisting}
void search(int low, int high) {
  if (low == high) search_base(low);
  else {
    int mid = (low+high)/2;
    #pragma omp task
    search(low, mid);
    #pragma omp task
    search(mid + 1, high);
    #pragma omp taskwait
  } 
}
\end{lstlisting}
\vspace{0.1ex}
\end{tabular*}

\caption[Example of common-subexpression elimination on an OpenMP
    program.]{Example of common-subexpression elimination on an OpenMP
    program.  \subfigcap{a}~The function \code{search}, which uses
    parallel divide-and-conquer to apply the function
    \code{search_base} to every integer in the closed interval
    [\code{low}, \code{high}].  \subfigcap{b}~An optimized version of
    \code{search}, where the common subexpression \code{(low+high)/2}
    of the original version
    is computed only once and stored in the variable \code{mid} in
    the optimized version.}
  \label{fig:search}
\end{figure}

Consider the first code segment defined in \figref{search}.
Here, we find a simple OpenMP program which performs a divide-and-conquer search.
However, as written, there is a simple optimization that can be performed --
namely, that the value \code{(low+high)/2} may be computed once before either
parallel task, allowing the program to avoid some redundant computation. On
the serial version of this program, optimizations like this are performed
automatically. However, without the use of Tapir, these sorts of optimizations
are not run on parallel programs.

If one were to compile this program using the traditional technique of converting
to runtime calls, one would find a program similar to \figref{runtime_calls}.
The OpenMP pragmas are effectively treated as syntactic sugar for such runtime calls
because vanilla LLVM has no concise way to represent parallel programs.
These runtime calls obfuscate the program, making it infeasable to analyze or
optimize parallel code.

\begin{figure}[t]
\begin{tabular*}{\linewidth}{@{\extracolsep{\fill}}l@{}l}
\begin{lstlisting}
void task_1(int* clos) {
  search(*clos[0], (*clos[0] + *clos[1])/2);
}

void task_2(int* clos) {
  search(*clos[0], (*clos[0] + *clos[1])/2);
}

void search(int low, int high) {
  if (low == high) search_base(low);
  else {
    int* closure_1[] = {&low, &high};
    omp_run_task(task_1, closure_1);
    int* closure_2[] = {&low, &high};
    omp_run_task(task_2, closure_2);
    omp_taskwait();
  } 
}
\end{lstlisting}
\vspace{0.1ex}
\end{tabular*}

\caption[Simplified compilation of the unoptimized search code from \figref{search}.]{Simplified compilation of the unoptimized search code from \figref{search}.  \subfigcap{a}~The function \code{search}. The parallel tasks are moved into their own functions and accompanying closures. These functions are then passed to OpenMP runtime calls. This system obfuscates the program, making it infeasable for an optimizer to reason about what is happening.}
  \label{fig:runtime_calls}
\end{figure}

\subsection{Tapir Internals}
Tapir is a compelling solution to this problem because it allows existing
analysis and optimizations to work on parallel programs without much modification.
To properly describe Tapir, one must briefly introduce key aspects of LLVM.
LLVM represents control flow through a control-flow graph (CFG) of blocks,
which are sequences of instructions. Blocks always end with terminator instructions
which define the edges of the control flow graph. For example, \figref{CFG} shows a simple
function and its corresponding LLVM code.

\begin{figure*}[h!]
  \begin{tabular*}{\linewidth}{@{\extracolsep{\fill}}ll}

    \subfiglabel{a} & \subfiglabel{b} \\
\begin{minipage}[T]{0.45\linewidth}
      \begin{lstlisting}
int f(int x) {
  int y = 10;
  if (x == 0) {
    y = 12;
  }
  return y;
}
      \end{lstlisting}
    \end{minipage}
&
    \begin{minipage}[T]{0.45\linewidth}
\begin{lstlisting}[language=llvm]
define i32 @f(i32 %x) {
entry:
  %y = alloca i32, align 4
  store i32 10, i32* %y, align 4
  %cmp = icmp eq i32 %x, 0
  br i1 %cmp, label %if.then, label %if.end

if.then:
  store i32 12, i32* %y, align 4
  br label %if.end

if.end:
  %1 = load i32, i32* %y, align 4
  ret i32 %1
}
\end{lstlisting}
    \end{minipage}\\
    \addlinespace[2ex]
    \bottomrule
  \end{tabular*}
  \caption{Example code snippet and the corresponding LLVM code. This code segment contains three blocks: \code{entry}, \code{if.then}, and \code{if.end}. The conditional statement is implemented in LLVM by the conditional branch at the end of the entry block. }
  \figlabel{CFG}
  \vspace{-.4cm}
\end{figure*}

To represent parallelism, Tapir introduces the \code{detach}, \code{reattach}, and
\code{sync} instructions. The \code{detach} statement is syntactically similar to
the conditional branch as it is also a terminator instruction with with two blocks.
However, unlike the conditional branch which denotes a choice between successors, the \code{detach} instruction denotes that its two successors may run in parallel. Semantically,
the first successor of a \code{detach} instruction is referred to as the \textbf{detached block} and represents a task that can, but is not required to, run in parallel with the
\textbf{continuation block}, or the second successor to the detach statement. The \code{detach}
is analagous to a \code{fork} in most fork-join frameworks.

At the end of any task created by a \code{detach} instruction, one must end with a \code{reattach} instruction to the corresponding continuation block. The \code{reattach} instruction
is used to denote the end of a parallel task.

Finally, one may use the \code{sync} instruction to wait for any outstanding tasks created
by \code{detach} instructions within the current function to complete. The \code{sync}
instruction is analagous to a \code{join} in most fork-join frameworks.


\section{OpenMP Tasks} \label{Sec:OpenMP}

Initially, the cross-vendor OpenMP~\cite{openmp} shared memory programming model 
focused on the execution of data parallelism by a cooperating team of threads, 
e.g., dividing the iterations of a loop among the threads. Version 3.0 of the 
OpenMP API specification introduced support for lightweight asynchronous tasks, 
designated by the application developer and scheduled onto the team of threads 
by the OpenMP run time implementation.  The \texttt{task} construct applied to 
a structured block of code creates an explicit task, and the \texttt{taskwait} 
construct waits for completion of all tasks generated by the current task.

Recursive task creations and synchronizations using the constructs result in 
an implicit directed acyclic graph (DAG) that allows both reasoning about and 
visualization of the program execution.  \figref{fib-graph-to-schedule}
shows some example code, a view of the task DAG, and a simplified execution 
schedule mapping the tasks to a team of two threads.  In the example, the Nth 
Fibonacci number is calculated by recursively generating tasks to calculate 
the (N-1)th and (N-2)th Fibonacci numbers.  The \texttt{taskwait} ensures that 
the child tasks have completed before their answers are combined to yield the 
final result.

\begin{figure*}
\begin{center}
\begin{tabular}{| c c c | c c c | c c c |}
\hline
 & 
\begin{lstlisting}
int fib(int n) 
{                    // A
  if (n < 2)
    return n;
  else
  {
    int x, y;
    #pragma omp task
      x = fib(n-1);  // B
    #pragma omp task
      y = fib(n-2);  // C
    #pragma omp taskwait
    return (x+y);    // D
  }
}
\end{lstlisting}
& & &
    \centering{
   \raisebox{-0.6in}{\includegraphics[scale=0.35]{fib-task-graph.pdf}}
    }
& & &
    \centering{
     \raisebox{-0.25in}{\includegraphics[scale=0.35]{fib-schedule.pdf}}
    }
 &  \\
\hline
\end{tabular}
\end{center}
\caption{Code, task graph, and schedule of a simple brute force recursive 
Fibonacci number calculation on two threads.}
\label{fig:fib-graph-to-schedule}
\end{figure*}

The initial design of the OpenMP task model~\cite{ayguade09design} established 
a basic framework for asynchronous task parallel execution in OpenMP programs.
Subsequent versions of the OpenMP specification up to the current version 4.5 
have added additional new features to the tasking model.  The \texttt{depend} 
clause codifies data dependences among tasks, indicating that a data location 
is an input or output of a task.  The run time system ensures that a task is 
not scheduled until its input dependences are fulfilled.  The \texttt{taskloop} 
construct combines groups of independent loop iterations into explicit tasks, 
enabling composition of concurrent loop execution and independent explicit 
tasks within the same OpenMP parallel region. The \texttt{taskgroup} construct 
waits on not only all child tasks, but all descendent tasks, providing a deep 
synchronization. The \texttt{taskwait} construct allows the application to 
indicate a point at which the implementation may suspend the current task to 
work on other tasks, as may be desired for long-running tasks that generate 
many others.  The task concept was also leveraged to provide for asynchronous 
offload of data and computation to accelerators by applying the \texttt{nowait} 
clause to device constructs like the \texttt{target} construct to generate an 
asynchronous \textit{target task}.

Several clauses for the \texttt{task} construct aim to optimize execution of 
tasks but can be safely ignored by an implementation that chooses to do so. 
The \texttt{mergeable} clause allows the implementation to omit creation of a 
new data environment for a descendent of a task marked with the \texttt{final}
clause.  The \texttt{priority} clause assigns an integer priority to the task 
and recommends the prioritization of tasks with higher priority values.
The \texttt{untied} clause allows a task to be migrated between threads after 
suspension, which enables practical use of \textit{work-first scheduling} 
(suspending the parent task in favor of executing each child task immediately 
on the thread where it is generated).

\section{Implementation} \label{Sec:Implementation}

Tapir is implemented as an extension to the LLVM instruction set. Clang is a C
family compiler that has support for OpenMP extensions \cite{clang} and targets
LLVM. The existing Clang OpenMP implementation maps OpenMP constructs directly
to OpenMP runtime library calls, by wrapping a C statement in a
\texttt{CapturedStmt}. This replaces a C statement with a function call into
the OpenMP runtime library, along with a machine generated function who's body
contains that statement.   

For this work, we replaced a subset of the OpenMP implementation to generate
Tapir IR instead of vanilla LLVM IR with OpenMP runtime calls. Specifically, we
replace the two primary pragmas for task parallelism: \texttt{task\_spawn} and
\texttt{task\_wait}. By re-using code from Schardl et al. for code generation
of Cilk constructs, we were able to easily generate Tapir code for these OpenMP
pragmas. The ease with which this was completed is a testament to the quality of
the Tapir implementation. 

While we did implement the codegen for the \texttt{task\_spawn} and
\texttt{task\_wait} constructs, it's worth noting here that even for these
pragmas, the implementation is incomplete. Currently, any clauses modifying the
behaviour of the pragmas is ignored. Probably the most common semantics this
will change of variable semantics, e.g. \texttt{shared} vs. \texttt{private}.
Surprisingly, this had little effect on the correctness of the entire Barcelona
OpenMP task suite. Indeed, it is likely that fixing this issue would increase
the performance of this work marginally, due to reducing the number of memory
copies for variables declared \texttt{private} in OpenMP code. This property
that behaviour wasn't changed significantly is worth revisiting, and we do so
in Section~\ref{Sec:Discussion}. 

This work represented in this paper is only frontend implementation, which
means we use the only existing backend for Tapir. The existing backend
generates code that calls into the Cilk runtime, \texttt{libcilkrts}. This has
implications for adhering to the OpenMP specification. For example, environment
variables such as \texttt{OMP\_NUM\_THREADS} are ignored, replaced by
\texttt{CILK\_NWORKERS}.  We discuss some of these issues below, and return to
address others in the future work section. 

There are other issues that had to be overcome to run full OpenMP programs
using Tapir as described above. Generally, in addition the the \texttt{task} 
and \texttt{taskwait} pragmas, a program has to contain initialization OpenMP
pragmas in order to run parallel code. A standard pattern for building
task-parallel OpenMP programs is to insert a \texttt{parallel} pragma to start
the necessary hardware threads, followed immediately by a \texttt{single}
pragma to have only one thread continue on the specified statement. Then the
other threads will get work from spawned tasks, instead of implicitly executing
the same code in a data parallel style. For the purpose of this paper, we took
the shortcut of replacing these pragmas with no-ops. This worked well for all
examples except for one, in which after the \texttt{parallel} and
\texttt{single} pragmas the top level task was called with an unnecessary
\texttt{task} pragma.  Because the wait was implicit and unhandled by our
implementation, the simple fix was to remove the \texttt{task} pragma. This was
the only change required to the source code. Our temporary no-op shortcut, of
course, doesn't follow the OpenMP specification, and should be addressed by
future work. See Section~\ref{Sec:Future} for a further discussion how how the 
implementation can be improved.

All code used for the implementation is available at \\
\texttt{https://github.com/lanl/openmpir-clang}. The version used for this
paper is tagged as LLVM17.

\subsection{Example} \label{Sec:Example}

To better understand how the OpenMP to Tapir compiler works, we turn to the
\texttt{fib} example from Section~\ref{Sec:OpenMP}. In Figure~\ref{Fig:Example}
we show what Tapir code is generated.  

\begin{figure*}
\begin{tabular}{| c c c | c c c |}
\hline
 & 
\begin{lstlisting}
int fib(int n) 
{                    // A
  if (n < 2)
    return n;
  else
  {
    int x, y;
    #pragma omp task
      x = fib(n-1);  // B
    #pragma omp task
      y = fib(n-2);  // C
    #pragma omp taskwait
    return (x+y);    // D
  }
}
\end{lstlisting}
& & &
\begin{lstlisting}[language=llvm]
if.end:                                 
  detach label %det.achd, label %det.cont

det.achd:                                 
  %2 = load i32, i32* %n.addr, align 4
  %sub = sub nsw i32 %2, 1
  %call = call i32 @fib(i32 %sub)
  store i32 %call, i32* %x, align 4
  reattach label %det.cont

det.cont:                                   
  detach label %det.achd1, label %det.cont4

det.achd1:                                
  %3 = load i32, i32* %n.addr, align 4
  %sub2 = sub nsw i32 %3, 2
  %call3 = call i32 @fib(i32 %sub2)
  store i32 %call3, i32* %y, align 4
  reattach label %det.cont4

det.cont4:                                  
  sync label %sync.continue
\end{lstlisting}
 &  \\
\hline
\end{tabular}

\caption{Compilation to Tapir}
\label{Fig:Example}
\end{figure*}

The listed Tapir LLVM IR code corresponds to lines 7 through 12 of the C source
code.  Upon entering the \texttt{else} branch, the code immediately detaches
the basic block corresponding to the call to \texttt{fib(n-1)}, labeled
\texttt{det.achd}. The continuation for the first call also immediately
detaches the basic block corresponding to the call of \texttt{fib(n-2)}.
Finally, the continuation for the second call immediately calls sync. This is
actually a nice example of code that will be optimized by Tapir. The LLVM code
shown in Figure~\ref{Fig:Example} is not optimized. With optimizataions turned
on, Tapir will replace a \texttt{detach} of a function call followed
immediately by \texttt{sync} with a simple function call, resulting in faster
code. 

\section{Evaluation} \label{Sec:Evaluation}

For evaluation we compared performance on the Barcelona OpenMP task suite
\cite{barcelona} to existing OpenMP implementations. The Barcelona OpenMP task
suite is a set of tests intended to test performance of OpenMP implementations
using both irregular and regular tasking. The benchmark suite has added an
unbalanced tree search benchmark since the paper was published. Unfortunately
we were unable to succesfully run it on any implementation, even for even
medium input sizes. We have therefore left it out of our evaluation. The
following descriptions are abridged versions taken verbatim from
\cite{barcelona}:

\begin{itemize}
\item \texttt{Alignment}: aligns all protein sequences from  an  input
file  against  every  other  sequence  using  the Myers and Miller 
algorithm. The alignments are scored and the best score for each pair is
provided as a result. The scoring method is  a  full  dynamic  programming
algorithm. It uses  a  weight matrix to score mismatches, and assigns
penalties for opening and extending gaps. The output is the best score for each
pair of them.
\item \texttt{FFT}: computes the one-dimensional Fast Fourier Transform
of a vector of n complex values using the Cooley-Tukey 
algorithm. This is a divide and conquer algorithm that  recursively  breaks
down a Discrete Fourier Transform (DFT) into many smaller DFT’s. In each of the
divisions multiple tasks are generated.
\item \texttt{Fibonacci}: computes the n'th fibonacci number using a  recursive
paralellization. While  not  representative  of  an efficient  fibonacci
computation  it  is  still  useful  because  it  is a simple test case of a
deep tree composed of very fine grain tasks.  
\item \texttt{Floorplan}: kernel computes the optimal floorplan distribution
of a number of cells. The algorithm gets an input file with  cell’s
description  and  it  returns  the  minimum  area  size which includes all
cells. This minimum area is found through a recursive branch and bound search.
We hierarchically generate tasks  for  each  branch  of  the  solution  space.
The  state  of  the algorithm needs to be copied into each newly created task
so they can proceed. This implies that additional synchronizations have been
introduced in the code to maintain the parent state alive.
\item \texttt{Health}: simulates the Columbian Health Care System. It
uses multilevel lists where each element in the structure  represents  a
village with  a  list  of  potential patients and one hospital. The hospital
has several double-linked lists representing the possible status of a patient
inside it (waiting, in assessment,   in   treatment   or   waiting   for
reallocation).  At  each time step  all  patients  are  simulated  according
with several probabilities (of getting sick, needing a convalescence treatment,
or  being  reallocated to  an  upper  level  hospital).  A  task  is  created
for  each  village being  simulated. Once the lower levels have been simulated
synchronization occurs. 
\item \texttt{NQueens}: computes  all  solutions  of  the n-queens
problem, whose objective is to find a placement for $n$ queens on an $n \;
\times \; n$ chessboard such that none of the queens attack any other. It uses
a backtracking search algorithm with pruning. A task is created for each step
of the solution.
\item \texttt{Sort}: sorts a random permutation of n 32-bit numbers with  a
fast  parallel  sorting  variation of  the  ordinary mergesort.  First, it
divides an array of elements in two halves, sorting  each half  recursively,
and  then  merging  the  sorted halves with a parallel divide-and-conquer
method rather than the  conventional  serial  merge.  Tasks are  used  for
each  split and merge. When the array is too small, a serial quicksort is used
so increase  the  task  granularity.  To  avoid  the overhead of  quicksort, an
insertion  sort  is  used  for  very  small  arrays (below a threshold of 20
elements).
\item \texttt{SparseLU}: computes an LU matrix factorization over
sparse matrices. A first level matrix is composed by pointers to  small
submatrices  that  may  not  be  allocated.  Due  to  the sparseness  of  the
matrix,  a  lot  of  imbalance  exists.  Matrix size and submatrix size can be
set at execution time. While a dynamic schedule can reduce the imbalance, a
solution with tasks parallelism seems to obtain better results. In each
of the sparseLU  phases,  a  task  is  created  for  each  block  of  the
matrix that is not empty.
\item \texttt{Strassen}: algorithm  uses  hierarchical  decomposition of a
matrix for multiplication of large dense matrices. Decomposition is done by
dividing each dimension of the matrix into  two  sections  of  equal size. For
each decomposition a task is created. 
\end{itemize}

There are two classes of input sizes for the benchmarks. Some, like Floorplan,
require input files. For each of these, we chose the largest provided. For the
benchmarks with a parameter to adjust the size of the input, we attempted to 
choose a parameter size so that the fastest implementation was on the order of
seconds. The one exception was for \texttt{Fibonacci}. Because, as discussed
further below, the Intel implementation required large stacksize, we stopped at
42. The exact inputs used are:

\begin{itemize}
\item \texttt{Alignment: -f prot.100.aa} 
\item \texttt{FFT: -n 335544320}
\item \texttt{Fibonacci: -n 42}
\item \texttt{Floorplan: -f input.20}
\item \texttt{Health: -f large.input}
\item \texttt{NQueens: -n 15}
\item \texttt{Sort -n 335544320}
\item \texttt{SparseLU: -n 100 -m 100}
\item \texttt{Strassen: -n 8192}
\end{itemize}

The Intel compiler required increasing the stacksize for \texttt{Fibonacci} and
\texttt{FFT}. The implementation seemed to use significantly more stack space
than the others, requiring setting \texttt{OMP\_STACKSIZE} to \texttt{256M} for
computing the 42nd Fibonacci number. \texttt{FFT} ran fine with an increase to
\texttt{16M}.  

For other implementations, we compare to GCC 7.1, Clang 4.0.1, and Intel
17.0.0. The machine used is a two socket Intel Xeon E5-2683 v3 machine with
132GB of memory, running Linux 4.11.4. Each Xeon has 16 cores running at
2.1GHz, with 20M of L3 cache. All benchmarks were run using all 64
hyper-threads. 32 thread tests were run as a sanity check and showed
qualitatively similar results. It's worth noting that each of the other
implementations comes with it's own runtime. In this sense, performance is a
function of both the compiler and any optimizations it's able to perform, and
the runtime.  Understanding the interaction of these two parts is non-trivial,
as we will see when discussing results. Each compiler was run with the flags
\texttt{-O3 -fopenmp}. Our compiler was also run with \texttt{-ftapir} to
enable Tapir instructions to be lowered to a parallel runtime (in this case,
the default Cilk runtime).

As mentioned in the Section~\ref{Sec:Implementation}, the gaps in the implementation
changed program behaviour in a couple cases. In the case of \texttt{FFT}, we were
forced to remove an unecessary \texttt{task} pragma, as the current implementation
doesn't insert the implicit barrier at the end of a \texttt{parallel} region. This 
was a one line fix in the benchmark, and would be fixed properly by handling
the \texttt{parallel} and \texttt{single} pragmas correctly in our implementation. 

The second change in program behaviour due to our incomplete implementation was
caused by the lack \texttt{critical} pragma and \texttt{atomic} pragmas. This
issue showed up in the \texttt{Floorplan} benchmark. The \texttt{critical} pragma
ensures that only one thread can executed the referred statement at a time. 
This should be fixable in the implementation by adding a simple code-localized
synchronization generation in the IR. Similarly, the \texttt{atomic} pragma 
can be addressed by retaining a pointer to the shared stack variable, much like
existing OpenMP implementations. It is worth noting that this behaviour was
non-deterministic, and that roughly half of the time the \texttt{Floorplan}
still returned correct results. As we will see, this makes for an interesting
trade-off given the performance increase witnessed.

We set a timeout of 10 minutes, as at least one implementation was always
finishing within 10 seconds, anything running more than 60 times slower becomes
irrelevant. Due to time constraints, correctness checks were not performed on
every run, so it is possible some non-determinism was missed. 

Each variant was run 10 times, with the height of the bars representing the
mean and the error bars representing standard deviation. In cases where there
was timeout or segmentation fault, no time is reported, and the run is marked
as faulty. 

\section{Results} \label{Sec:Results}

In this section we discuss the results of running our evaluation on our
implementation, and how its performance compared to the existing
implementations listed in Section~\ref{Sec:Evaluation}. See
Figures~\ref{Fig:results} and \ref{Fig:results2} for a simple visualization of
the performance. Each graph represents a single benchmark from the previously
enumerated Barcelona benchmarks, and each bar represents one of the
implementations. 

\begin{figure*}
\begin{multicols}{2}
  \includegraphics[width=\linewidth]{alignment.pdf} \par
  \includegraphics[width=\linewidth]{fib.pdf} 
  \includegraphics[width=\linewidth]{fft.pdf} \par
  \includegraphics[width=\linewidth]{floorplan.pdf} 
  \includegraphics[width=\linewidth]{sort.pdf} \par
  \includegraphics[width=\linewidth]{nqueens.pdf}
\end{multicols}
\caption{Barcelona OpenMP Task Suite Results}
\label{Fig:results}
\end{figure*}

\begin{figure*}
\begin{multicols}{2}
  \includegraphics[width=\linewidth]{health.pdf} \par
  \includegraphics[width=\linewidth]{sparselu.pdf} 
\end{multicols}
\centering
\includegraphics[width=0.45\linewidth]{strassen.pdf}
\caption{Barcelona OpenMP Task Suite Results (continued)}
\label{Fig:results2}
\end{figure*}

As mentioned earlier on \texttt{Floorplan} our implementation returned correct
results nondeterministically due to not having an implementation of the OpenMP
\texttt{critical} and \texttt{atomic} pragmas. While ICC and Clang finished in
roughly 1-2 seconds, the Tapir implementation was finishing in 0.02 seconds,
and computing the correct result roughly half the time. This raises interesting
questions on the cost/benefit relation of the missing OpenMP pragmas.

GCC was the only culprit for timeouts. Recall that the timeout was set at 10
minutes, so any timeout means GCC was running at least approximately 60 times
slower than the fastest implementation. For example, on the \texttt{FFT}
benchmark, while our Tapir implementation was running in under 5 seconds, the
GCC implementation was timing out at 600 seconds, so was at least 100 times
slower. We've left GCC results off of the graphs for the three timeout cases,
\texttt{FFT}, \texttt{Fibonacci}, and \texttt{NQueens}.

Putting failures aside, performance for our implementation is quite strong,
generally outperforming or matching the best of existing implementations. For
benchmarks where the overhead to work ratio is high is where the Tapir
implementation really shines. For example, for the \texttt{Fibonacci}
benchmark, our implementation finishes significantly faster than any of the
others. While ICC failed for that sized input, we did test with smaller inputs
and found its similarly outpaced by the Tapir implementation.  \texttt{NQueens}
and \texttt{FFT} show similar behaviour, to lesser degrees. In contrast,
benchmarks that have a lower overhead to work ratio unsurprisingly differ less
in performance. For example,  

\subsection{Profiling}

In this section we attempt to understand \emph{why} the performance varies in
the ways it does. While we can't hope to figure out every discrepancy in
performance, we can hope to get some ideas for why the performance of our
implementation is generally better, and where each of the implementations is
spending its time.

Our primary tool for this task will be performance counters. It's worth noting
that some information is difficult to infer from performance counters, such as
sources of contention in runtimes, reasons for memory locality issues, etc.
Still, it will give us some insight into performance bottlenecks.

Before we begin understanding why, it's worth noting again that the scope of the 
work that this paper is responsible for is only the frontend. All performance
gains due to the existing Tapir backend and Cilk runtime we sadly cannot claim
credit for. 

We can break reasons for performance discrepancies into a few categories: 

\begin{itemize}
\item Front-end code generation
\item IR Optimizations
\item Runtime efficiency
\end{itemize}

While we would like to distinguish between these, it is difficult to do so with
only performance counter information. A more thorough investigation would
require deep knowledge of each of the frontends and runtimes. While the
importance of optimizations of parallel code is clear, we hypothesize that most
of the disparity present in these results is a function of runtime
implementation differences. As justification for this hypothesis, consider
the \texttt{Fibonacci} benchmark. There are no memory accesses to the heap, 
and the code leaves little room for optimization, algorithmic changes
notwithstanding. In other words, it is effectively a benchmark of how fast a
runtime can execute fork-join parallelism. \gsnote{still worth looking into the
optimization I mention in the implemtnation section.}

%\wmnote{So actually the memory accesses are happily an optimization Tapir does
%-- mainly some mem2reg optimization that may not make much of a different
%(though still something) on one core, is a lot more impactful on multicores
%because of issues regarding contention forcing a cache line flush.  Also I'm
%not convinced there wasn't an impact on FFT/nqueens because we did see a decent
%improvement from tapir on that -- though I wouldn't be suprised to see the
%runtime contribute a large chunk of that, especially for nqueens because that
%seems like a much larger improvement than the serial optimizations applied to
%parallel program alone (when we tested).  Fibonacci is certainly runtime based
%because there's basically no work inside of it so its almost entirely a
%measurement of the runtime efficiency.  Sort I'm not sure on depending on the
%implementation
%}

There are two cases we'd like to look at using the profiler. The first is a 
case when there is a large difference in runtime between implementations. For 
this case, our hypothesis is that in these cases the slower implementations
are, for some reason, spending more time in the runtime. As mentioned above, 
it is difficult to understand exactly why an implementation is spending more 
time in the runtime without further investigation, but it is a useful 
sanity check nonetheless. For this case, we turn to the \texttt{Fib} benchmark.
Indeed, we get roughly the following breakdowns of work to overhead ratio
according to the Linux \texttt{perf} tool:

\begin{itemize}
\item \texttt{tapir: ~30\% runtime overhead, ~50\% work, ~20\% other}
\item \texttt{clang: ~85\% runtime overhead, ~5\% work, ~10\% other}
\item \texttt{icc: ~85\% runtime overhead, ~5\% work, ~10\% other}
\item \texttt{gcc: ~100\% runtime overhead, ~0\% work, ~0\% other}
\end{itemize}

This supports our hypothesis that most of the slowdown incurred is overhead in
the runtime. For \texttt{tapir}, the overhead is relatively small given the
size of the work chunks, for \texttt{clang} and \texttt{icc} it is
significantly larger, while for \texttt{gcc} it is completely overwhelming.

For the cases where performance is close between implementations, like
\texttt{sparselu}, we expect runtimes to be dominated by actual task work. This 
expectation is confirmed by investigating performance counters for \texttt{sparselu}:
every implementation spends roughly 90\% of it's time doing work in the task
bodies. 

\section{Discussion} \label{Sec:Discussion}

In this section we discuss the relevance of the results and this work in general
in the context of the literature. As discussed in Section~\ref{Sec:Introduction}, 
there have been many programming models for implementing parallel programs. While
this paper does not take a stance on which one \emph{should} be used, it does
accept that OpenMP is currently a widely used model, potentially the most
widely used model that has a notion of tasks. 

The goal of this work is to show that a single intermediate representation can be
used to implement multiple programming models, with good performance. Schardl et 
al. showed that Tapir works well as an IR for Cilk, and surmised that it could
be an effective IR for a large subset of OpenMP \cite{tapir}. This work has
taken the first step in that direction, showing that it can work well as an IR
for OpenMP tasks. 

One interesting aspect of this work is that it compiles a language extension intended
for one runtime (OpenMP runtimes) to use another (the Cilk runtime). While in many
ways this is a weakness of this work, it does open the possibility of doing this 
in a general way. Having a real IR to work with enables, in the same way that LLVM
does for serial code, the possibility of multiple input parallel languages and
multiple output parallel runtimes. While the OpenMP spec's requirement to
have access to runtime routines complicates things, for many programming models
it seems at least conceivable to map them onto multiple different runtimes. This
opens the possibility of mixing programming models at compile time by targeting
the same runtime. For example, if both OpenMP and Cilk targeted Tapir
instructions, one could have a large application where some parts were written
using Cilk, while other used OpenMP, and the result would not suffer the
incompatibility problems often seen today. 

The surprising fact that program behaviour wasn't changed significantly is
worth discussing here. We surmise that it is likely due to the variable
semantics borrowed from the Cilk codegen and their relation to the OpenMP
specification of semantics. In particular, \emph{OpenMP doesn't specify any
memory model for shared variables}. This results in the Cilk style of only
writing to a shared variable at the end of a parallel section as a perfectly
valid implementation of OpenMP's specification. In contrast, existing OpenMP
implementations often write to shared variables on every write. This is a
potential for performance discrepancy due to differing implementation-defined
behaviour, and we will return to it when discussing performance on the
Barcelona OpenMP task suite. There is of course the fact that technically, by
copying the value of variables back to the surrounding context even for
\texttt{private} variables, one isn't following the specification. We surmise
that in practice this hasn't had an effect due to the fact that
\texttt{private} variables are generally used for performance, rather than
their semantic properties. 

\subsection{Future Work} \label{Sec:Future}

One of the important questions for this work is whether or not the Tapir
instructions can be extended to cover the full OpenMP semantics. As shown
in this paper, even the full semantics of OpenMP tasks weren't covered. Runtime
calls aside, implementing the full semantics of OpenMP pragmas would be a
significant undertaking. 

As two examples, let us consider the problematic \texttt{atomic} and
\texttt{critical} pragmas from the \texttt{Floorplan} benchmark. The 
\texttt{atomic} pragma would likely not be too difficult to translate
from existing OpenMP codegen. One would have to simply keep around and then
modify the contents of the original stack pointer to the shared variable. The
\texttt{critical} section would likely be a little more difficult. Still, it
should be possible to modify existing OpenMP code to block on that section. 

Still, whether or not it's \emph{possible} to implement these extra features
is a separate question from whether or not it's \emph{useful}. If implementing
extra features require using special synchronization primitives, or calls into
runtimes, or any other tool other than the Tapir instructions, then the
compiler cannot reason about code using those features, and many of the
advantages we've discussed become moot, e.g. backend compatibility, generic 
optimization, etc. Features like the OpenMP \texttt{atomic} pragma don't 
require any changes to control flow, and therefore are likely to work well 
with Tapir analyses and optimizations. On the other hand features like OpenMP's
\texttt{critical} section \emph{do} require changes to control flow that seem
challenging at best to map onto Tapir's instructions. Furthermore, even if it
is possible to implement features using the Tapir instructions, it may require 
such radical entangling of the code that even if analysis is technically
possible, it becomes infeasible. 

An obvious first step towards better OpenMP coverage, other than the features
listed above, is OpenMP \texttt{parallel for} loops. These are similar enough
to \texttt{cilk\_for} loops in semantics that this also should be a relatively
painless next step to adapt from the existing Cilk implementation. 

While OpenMP and Cilk have similar fork-join style semantics that Tapir
implements naturally, many allow a more flexible mechanism for synchronization:
futures \cite{qthreads, chapel, hpx}. The primary difference between the
fork-join parallelism represented by Tapir and these approaches is that
child tasks can outlive their parents. Figuring out how to represent this class
of parallelism in Tapir represents a significant challenge for future work.

A seperate but related area in LLVM extensions is the area of using memory 

\subsection{Related Work} \label{Sec:Related}

There has been significant work on internal representations for parallel
programs. These can be roughly broken into four categories. 

First, compilers can attach metadata to an existing IR. For example, LLVM has a
parallel loop metadata construct \cite{}. This has the benefit of being
flexible and requiring minimal work, but historically can be lost during
optimizations, and any code movement can break the semantics. The flexibility
of this approach can also be viewed as a negative, as it can compromise
what can otherwise be a simple, well defined semantics for the IR. 

A second approach is to use intrinsic functions to define parallel tasks
\cite{ares}. This, like the metadata approach, has the advantage of being
flexible and relatively easy to implement, but at the cost of being less well
defined. There has recently been proposals to standardize intrinsics extensions
for LLVM to represent parallelism, but we would caution that a large, flexible
set of intrinsics being added to an already only-partially defined
\cite{verillvm} language could make reasoning about parallel programs, even
in an IR settings, infeasible. 

A third approach is to have a separate instruction set for parallelism. This is
the approach taken by projects like HPIR \cite{zhao2011intermediate}, SPIRE
\cite{khaldi2012spire}, and INSPIRE \cite{jordan2013inspire}. This approach has
the downside of being unable to re-use existing optimization and analysis
infrastructure of an existing IR. 

Finally, the last approach, and that taken by Tapir, is to implement parallel 
instructions as an extension of an existing IR. This allows for integration
into existing analyses and optimizations. In the case of Tapir, this allows one
to leverage years of development into program analysis and optimization,
extending only where necessary. 

While there is general agreement that there needs to be IR support for
parallelism in parallel programs, there isn't consensus on which of these
approaches is best. Approaches that are easier at first, such as metadata
or intrinsic approaches, are similarly easy to extend to be unwieldy. There is
a reason that it's difficult to add instructions to LLVM: any added IR
construct needs to be considered in many locations. From debugging by printing
out IR, to case analysis for different control flow constructs, having 
a proper set of instructions to denote parallelism has a lot of advantages. 
Additionally, any hope of having a formal semantics for an IR depends on a 
simple, concise set of parallelism constructs, something that metadata and
intrinsics approaches can easily avoid. 

\section{Conclusion} \label{Sec:Conclusion}
We have shown that Tapir is an excellent IR target for OpenMP tasks. While not
all of the semantics are covered, we have a path forward for many of them. We have
shown that compiling to Tapir instructions allows for a straightforward compilation of 
OpenMP tasking programs to use the Cilk runtime system. We've also shown that this
combination leads to better performance than existing OpenMP tasking implementations
on the Barcelona OpenMP Tasking benchmark suite. Moving forward, we hope efforts like
this one help to reduce the fragmentation of parallel runtimes, as well as make
it easier to write optimization for parallel programs. 

\section*{Acknowledgments}
Sandia National Laboratories is a multimission laboratory managed and operated 
by National Technology and Engineering Solutions of Sandia, LLC., a wholly 
owned subsidiary of Honeywell International, Inc., for the U.S. Department of 
Energy's National Nuclear Security Administration under contract DE-NA-0003525.


This work was supported by the Director, Office of Advanced Scientific
Computing Research, Office of Science, of the United States Department
of Energy, under the guidance of Dr. Sonia Sachs. Los Alamos National
Laboratory is operated by Los Alamos National Security LLC for the U.S.
Department of Energy under contract DE-AC52-06NA25396.

This research was supported by US National Science Foundation under CRII
ACI 1565338, CNS 1527076, and CNS 1217948. Los Alamos National
Laboratory is operated by Los Alamos National Security LLC for the US
Department of Energy under contract DE-AC52-06NA25396.

Work supported by the Advanced Simulation and Computing program of the
U.S. Department of Energy's NNSA\@.  Los Alamos National Laboratory is
managed and operated by Los Alamos National Security, LLC (LANS), under
contract number DE-AC52-06NA25396 for the Department of Energy's
National Nuclear Security Administration (NNSA).

\bibliographystyle{ACM-Reference-Format}
\bibliography{annotated}

\end{document}
